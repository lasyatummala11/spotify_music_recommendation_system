{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f4631d8-4806-4d0b-a152-0a1aa8751eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.2.2-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\stsc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Downloading tzdata-2025.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\stsc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.2.3-cp313-cp313-win_amd64.whl (11.5 MB)\n",
      "   ---------------------------------------- 0.0/11.5 MB ? eta -:--:--\n",
      "   -------- ------------------------------- 2.4/11.5 MB 15.2 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 5.0/11.5 MB 13.7 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.9/11.5 MB 16.2 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.7/11.5 MB 14.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.5/11.5 MB 13.4 MB/s eta 0:00:00\n",
      "Downloading numpy-2.2.2-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "   ------ --------------------------------- 2.1/12.6 MB 10.3 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 5.0/12.6 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 7.9/12.6 MB 13.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 10.5/12.6 MB 13.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.6 MB 12.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 11.7 MB/s eta 0:00:00\n",
      "Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "Downloading tzdata-2025.1-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.2.2 pandas-2.2.3 pytz-2025.1 tzdata-2025.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The scripts f2py.exe and numpy-config.exe are installed in 'C:\\Users\\STSC\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: C:\\Users\\STSC\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "662a9c02-b7a5-4f8b-b30d-aa8242569025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\stsc\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for implicit (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [505 lines of output]\n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja (Visual Studio 17 2022 x64 v144)' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  -- The C compiler identification is unknown\n",
      "  CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "    No CMAKE_C_COMPILER could be found.\n",
      "  \n",
      "    Tell CMake where to find the compiler by setting either the environment\n",
      "    variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "    the compiler, or to the compiler name if it is in the PATH.\n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja (Visual Studio 17 2022 x64 v144)' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Visual Studio 17 2022 x64 v144' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "    Generator\n",
      "  \n",
      "      Visual Studio 17 2022\n",
      "  \n",
      "    could not find any instance of Visual Studio.\n",
      "  \n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Visual Studio 17 2022 x64 v144' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja (Visual Studio 17 2022 x64 v143)' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  -- The C compiler identification is unknown\n",
      "  CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "    No CMAKE_C_COMPILER could be found.\n",
      "  \n",
      "    Tell CMake where to find the compiler by setting either the environment\n",
      "    variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "    the compiler, or to the compiler name if it is in the PATH.\n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja (Visual Studio 17 2022 x64 v143)' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Visual Studio 17 2022 x64 v143' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "    Generator\n",
      "  \n",
      "      Visual Studio 17 2022\n",
      "  \n",
      "    could not find any instance of Visual Studio.\n",
      "  \n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Visual Studio 17 2022 x64 v143' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja (Visual Studio 16 2019 x64 v142)' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  -- The C compiler identification is unknown\n",
      "  CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "    No CMAKE_C_COMPILER could be found.\n",
      "  \n",
      "    Tell CMake where to find the compiler by setting either the environment\n",
      "    variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "    the compiler, or to the compiler name if it is in the PATH.\n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja (Visual Studio 16 2019 x64 v142)' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Visual Studio 16 2019 x64 v142' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "    Generator\n",
      "  \n",
      "      Visual Studio 16 2019\n",
      "  \n",
      "    could not find any instance of Visual Studio.\n",
      "  \n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Visual Studio 16 2019 x64 v142' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Ninja (Visual Studio 15 2017 x64 v141)' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  -- The C compiler identification is unknown\n",
      "  CMake Error at CMakeLists.txt:3 (ENABLE_LANGUAGE):\n",
      "    No CMAKE_C_COMPILER could be found.\n",
      "  \n",
      "    Tell CMake where to find the compiler by setting either the environment\n",
      "    variable \"CC\" or the CMake cache entry CMAKE_C_COMPILER to the full path to\n",
      "    the compiler, or to the compiler name if it is in the PATH.\n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Ninja (Visual Studio 15 2017 x64 v141)' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'Visual Studio 15 2017 x64 v141' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "    Generator\n",
      "  \n",
      "      Visual Studio 15 2017\n",
      "  \n",
      "    could not find any instance of Visual Studio.\n",
      "  \n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'Visual Studio 15 2017 x64 v141' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v144)' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "    Running\n",
      "  \n",
      "     'nmake' '-?'\n",
      "  \n",
      "    failed with:\n",
      "  \n",
      "     no such file or directory\n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v144)' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v143)' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "    Running\n",
      "  \n",
      "     'nmake' '-?'\n",
      "  \n",
      "    failed with:\n",
      "  \n",
      "     no such file or directory\n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'NMake Makefiles (Visual Studio 17 2022 x64 v143)' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'NMake Makefiles (Visual Studio 16 2019 x64 v142)' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "    Running\n",
      "  \n",
      "     'nmake' '-?'\n",
      "  \n",
      "    failed with:\n",
      "  \n",
      "     no such file or directory\n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'NMake Makefiles (Visual Studio 16 2019 x64 v142)' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "  \n",
      "  \n",
      "  --------------------------------------------------------------------------------\n",
      "  -- Trying 'NMake Makefiles (Visual Studio 15 2017 x64 v141)' generator\n",
      "  --------------------------------\n",
      "  ---------------------------\n",
      "  ----------------------\n",
      "  -----------------\n",
      "  ------------\n",
      "  -------\n",
      "  --\n",
      "  Not searching for unused variables given on the command line.\n",
      "  CMake Deprecation Warning at CMakeLists.txt:1 (cmake_minimum_required):\n",
      "    Compatibility with CMake < 3.10 will be removed from a future version of\n",
      "    CMake.\n",
      "  \n",
      "    Update the VERSION argument <min> value.  Or, use the <min>...<max> syntax\n",
      "    to tell CMake that the project requires at least <min> but has been updated\n",
      "    to work with policies introduced by <max> or earlier.\n",
      "  \n",
      "  \n",
      "  CMake Error at CMakeLists.txt:2 (PROJECT):\n",
      "    Running\n",
      "  \n",
      "     'nmake' '-?'\n",
      "  \n",
      "    failed with:\n",
      "  \n",
      "     no such file or directory\n",
      "  \n",
      "  \n",
      "  -- Configuring incomplete, errors occurred!\n",
      "  --\n",
      "  -------\n",
      "  ------------\n",
      "  -----------------\n",
      "  ----------------------\n",
      "  ---------------------------\n",
      "  --------------------------------\n",
      "  -- Trying 'NMake Makefiles (Visual Studio 15 2017 x64 v141)' generator - failure\n",
      "  --------------------------------------------------------------------------------\n",
      "  \n",
      "                  ********************************************************************************\n",
      "                  scikit-build could not get a working generator for your system. Aborting build.\n",
      "  \n",
      "                  Building windows wheels requires Microsoft Visual Studio.\n",
      "              Get it from:\n",
      "  n\n",
      "               https://visualstudio.microsoft.com/vs/\n",
      "  \n",
      "                  ********************************************************************************\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for implicit\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  Building wheel for scikit-surprise (pyproject.toml) did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [115 lines of output]\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\accuracy.py -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\builtin_datasets.py -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\dataset.py -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\dump.py -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\reader.py -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\trainset.py -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\utils.py -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\__init__.py -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\__main__.py -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  creating build\\lib.win-amd64-cpython-312\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\search.py -> build\\lib.win-amd64-cpython-312\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\split.py -> build\\lib.win-amd64-cpython-312\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\validation.py -> build\\lib.win-amd64-cpython-312\\surprise\\model_selection\n",
      "  copying surprise\\model_selection\\__init__.py -> build\\lib.win-amd64-cpython-312\\surprise\\model_selection\n",
      "  creating build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\algo_base.py -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\baseline_only.py -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\knns.py -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\predictions.py -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\random_pred.py -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\__init__.py -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  running egg_info\n",
      "  writing scikit_surprise.egg-info\\PKG-INFO\n",
      "  writing dependency_links to scikit_surprise.egg-info\\dependency_links.txt\n",
      "  writing entry points to scikit_surprise.egg-info\\entry_points.txt\n",
      "  writing requirements to scikit_surprise.egg-info\\requires.txt\n",
      "  writing top-level names to scikit_surprise.egg-info\\top_level.txt\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\arrayscalars.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarrayobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ndarraytypes.h won't be automatically included in the manifest: the path must be relative\n",
      "  dependency C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\numpy\\core\\include\\numpy\\ufuncobject.h won't be automatically included in the manifest: the path must be relative\n",
      "  reading manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  reading manifest template 'MANIFEST.in'\n",
      "  warning: no previously-included files matching '*.so' found under directory 'surprise'\n",
      "  adding license file 'LICENSE.md'\n",
      "  writing manifest file 'scikit_surprise.egg-info\\SOURCES.txt'\n",
      "  C:\\Users\\STSC\\AppData\\Local\\Temp\\pip-build-env-ulf3aneh\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:212: _Warning: Package 'surprise.prediction_algorithms' is absent from the `packages` configuration.\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          ############################\n",
      "          # Package would be ignored #\n",
      "          ############################\n",
      "          Python recognizes 'surprise.prediction_algorithms' as an importable package[^1],\n",
      "          but it is absent from setuptools' `packages` configuration.\n",
      "  \n",
      "          This leads to an ambiguous overall configuration. If you want to distribute this\n",
      "          package, please make sure that 'surprise.prediction_algorithms' is explicitly added\n",
      "          to the `packages` configuration field.\n",
      "  \n",
      "          Alternatively, you can also rely on setuptools' discovery methods\n",
      "          (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
      "          instead of `find_packages(...)`/`find:`).\n",
      "  \n",
      "          You can read more about \"package discovery\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
      "  \n",
      "          If you don't want 'surprise.prediction_algorithms' to be distributed and are\n",
      "          already explicitly excluding 'surprise.prediction_algorithms' via\n",
      "          `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
      "          you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
      "          combination with a more fine grained `package-data` configuration.\n",
      "  \n",
      "          You can read more about \"package data files\" on setuptools documentation page:\n",
      "  \n",
      "          - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
      "  \n",
      "  \n",
      "          [^1]: For Python, any directory (with suitable naming) can be imported,\n",
      "                even if it does not contain any `.py` files.\n",
      "                On the other hand, currently there is no concept of package data\n",
      "                directory, all directories are treated like packages.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    check.warn(importable)\n",
      "  copying surprise\\similarities.c -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\similarities.pyx -> build\\lib.win-amd64-cpython-312\\surprise\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.c -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.c -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.c -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.c -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\co_clustering.pyx -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\matrix_factorization.pyx -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\optimize_baselines.pyx -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  copying surprise\\prediction_algorithms\\slope_one.pyx -> build\\lib.win-amd64-cpython-312\\surprise\\prediction_algorithms\n",
      "  running build_ext\n",
      "  building 'surprise.similarities' extension\n",
      "  error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for scikit-surprise\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (implicit, scikit-surprise)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\stsc\\anaconda3\\lib\\site-packages (2.2.2)\n",
      "Collecting surprise\n",
      "  Downloading surprise-0.1-py2.py3-none-any.whl.metadata (327 bytes)\n",
      "Collecting implicit\n",
      "  Using cached implicit-0.7.2.tar.gz (70 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: scipy in c:\\users\\stsc\\anaconda3\\lib\\site-packages (1.13.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Collecting scikit-surprise (from surprise)\n",
      "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from implicit) (4.66.5)\n",
      "Requirement already satisfied: threadpoolctl in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from implicit) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tqdm>=4.27->implicit) (0.4.6)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from scikit-surprise->surprise) (1.4.2)\n",
      "Downloading surprise-0.1-py2.py3-none-any.whl (1.8 kB)\n",
      "Building wheels for collected packages: implicit, scikit-surprise\n",
      "  Building wheel for implicit (pyproject.toml): started\n",
      "  Building wheel for implicit (pyproject.toml): finished with status 'error'\n",
      "  Building wheel for scikit-surprise (pyproject.toml): started\n",
      "  Building wheel for scikit-surprise (pyproject.toml): finished with status 'error'\n",
      "Failed to build implicit scikit-surprise\n"
     ]
    }
   ],
   "source": [
    "pip install numpy pandas surprise implicit scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81651470-38f8-4e3f-9588-cff519c87b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install implicit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "997688de-24a5-4083-b4ab-6c81b3ce2461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faiss-cpuNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from faiss-cpu) (1.26.4)\n",
      "Requirement already satisfied: packaging in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from faiss-cpu) (24.1)\n",
      "Downloading faiss_cpu-1.10.0-cp312-cp312-win_amd64.whl (13.7 MB)\n",
      "   ---------------------------------------- 0.0/13.7 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 1.0/13.7 MB 6.3 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.9/13.7 MB 8.0 MB/s eta 0:00:02\n",
      "   ------------ --------------------------- 4.2/13.7 MB 7.4 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 5.5/13.7 MB 7.1 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 7.6/13.7 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 9.2/13.7 MB 7.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 11.0/13.7 MB 7.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 12.1/13.7 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  13.6/13.7 MB 7.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 13.7/13.7 MB 7.3 MB/s eta 0:00:00\n",
      "Installing collected packages: faiss-cpu\n",
      "Successfully installed faiss-cpu-1.10.0\n"
     ]
    }
   ],
   "source": [
    "pip install faiss-cpu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37df0652-a1a8-453b-a9ea-06b35d24063f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\stsc\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.13_qbz5n2kfra8p0\\localcache\\local-packages\\python313\\site-packages (from scikit-learn) (2.2.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.1-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp313-cp313-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 19.7 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.3/11.1 MB 17.8 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.1 MB 16.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 15.3 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.15.1-cp313-cp313-win_amd64.whl (43.6 MB)\n",
      "   ---------------------------------------- 0.0/43.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.8/43.6 MB 9.9 MB/s eta 0:00:05\n",
      "   -- ------------------------------------- 2.9/43.6 MB 7.0 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 7.3/43.6 MB 11.9 MB/s eta 0:00:04\n",
      "   ---------- ----------------------------- 11.0/43.6 MB 13.8 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 12.8/43.6 MB 12.7 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 17.8/43.6 MB 14.4 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 23.1/43.6 MB 16.1 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 28.0/43.6 MB 16.9 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 33.6/43.6 MB 17.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 38.5/43.6 MB 18.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/43.6 MB 19.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.6/43.6 MB 18.6 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0\n",
      "[notice] To update, run: C:\\Users\\STSC\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2414d283-ec24-4bff-a52b-353bf7390653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached flatbuffers-25.1.24-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (75.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.14.1)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (3.11.0)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow) (0.44.0)\n",
      "Requirement already satisfied: rich in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (13.7.1)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading optree-0.14.0-cp312-cp312-win_amd64.whl.metadata (48 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow) (2025.1.31)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.4.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.15.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\stsc\\anaconda3\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (0.1.0)\n",
      "Downloading tensorflow-2.18.0-cp312-cp312-win_amd64.whl (7.5 kB)\n",
      "Downloading tensorflow_intel-2.18.0-cp312-cp312-win_amd64.whl (390.3 MB)\n",
      "   ---------------------------------------- 0.0/390.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.6/390.3 MB 16.7 MB/s eta 0:00:24\n",
      "    --------------------------------------- 5.8/390.3 MB 18.5 MB/s eta 0:00:21\n",
      "    --------------------------------------- 8.9/390.3 MB 15.4 MB/s eta 0:00:25\n",
      "   - -------------------------------------- 11.5/390.3 MB 15.3 MB/s eta 0:00:25\n",
      "   - -------------------------------------- 12.8/390.3 MB 13.2 MB/s eta 0:00:29\n",
      "   - -------------------------------------- 16.3/390.3 MB 13.8 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 19.4/390.3 MB 14.1 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 22.3/390.3 MB 14.1 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 23.9/390.3 MB 13.7 MB/s eta 0:00:27\n",
      "   -- ------------------------------------- 25.7/390.3 MB 12.9 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 25.7/390.3 MB 12.9 MB/s eta 0:00:29\n",
      "   -- ------------------------------------- 27.0/390.3 MB 11.1 MB/s eta 0:00:33\n",
      "   -- ------------------------------------- 28.8/390.3 MB 11.0 MB/s eta 0:00:33\n",
      "   --- ------------------------------------ 32.0/390.3 MB 11.3 MB/s eta 0:00:32\n",
      "   --- ------------------------------------ 35.1/390.3 MB 11.5 MB/s eta 0:00:31\n",
      "   --- ------------------------------------ 38.8/390.3 MB 12.0 MB/s eta 0:00:30\n",
      "   ---- ----------------------------------- 44.0/390.3 MB 12.7 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 45.9/390.3 MB 12.5 MB/s eta 0:00:28\n",
      "   ---- ----------------------------------- 48.2/390.3 MB 12.5 MB/s eta 0:00:28\n",
      "   ----- ---------------------------------- 52.2/390.3 MB 12.8 MB/s eta 0:00:27\n",
      "   ----- ---------------------------------- 57.1/390.3 MB 13.3 MB/s eta 0:00:26\n",
      "   ------ --------------------------------- 61.1/390.3 MB 13.6 MB/s eta 0:00:25\n",
      "   ------ --------------------------------- 64.7/390.3 MB 13.8 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 68.4/390.3 MB 13.9 MB/s eta 0:00:24\n",
      "   ------- -------------------------------- 71.3/390.3 MB 14.1 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 74.4/390.3 MB 14.0 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 77.1/390.3 MB 14.0 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 80.2/390.3 MB 14.0 MB/s eta 0:00:23\n",
      "   -------- ------------------------------- 84.4/390.3 MB 14.2 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 88.3/390.3 MB 14.4 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 90.2/390.3 MB 14.3 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 92.8/390.3 MB 14.2 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 94.6/390.3 MB 14.0 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 96.7/390.3 MB 13.9 MB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 97.8/390.3 MB 13.8 MB/s eta 0:00:22\n",
      "   ---------- ----------------------------- 99.4/390.3 MB 13.5 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 100.7/390.3 MB 13.5 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 101.7/390.3 MB 13.1 MB/s eta 0:00:23\n",
      "   ---------- ---------------------------- 104.3/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ---------- ---------------------------- 106.7/390.3 MB 13.0 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 110.4/390.3 MB 13.1 MB/s eta 0:00:22\n",
      "   ----------- --------------------------- 113.8/390.3 MB 13.3 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 116.9/390.3 MB 13.3 MB/s eta 0:00:21\n",
      "   ----------- --------------------------- 120.1/390.3 MB 13.4 MB/s eta 0:00:21\n",
      "   ------------ -------------------------- 123.5/390.3 MB 13.4 MB/s eta 0:00:20\n",
      "   ------------ -------------------------- 127.9/390.3 MB 13.6 MB/s eta 0:00:20\n",
      "   ------------- ------------------------- 131.6/390.3 MB 13.7 MB/s eta 0:00:19\n",
      "   ------------- ------------------------- 134.5/390.3 MB 13.7 MB/s eta 0:00:19\n",
      "   ------------- ------------------------- 138.1/390.3 MB 13.7 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 140.8/390.3 MB 13.7 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 143.4/390.3 MB 13.7 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 145.0/390.3 MB 13.6 MB/s eta 0:00:19\n",
      "   -------------- ------------------------ 147.3/390.3 MB 13.5 MB/s eta 0:00:18\n",
      "   -------------- ------------------------ 149.9/390.3 MB 13.5 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 152.6/390.3 MB 13.5 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 155.5/390.3 MB 13.5 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 158.1/390.3 MB 13.5 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 158.3/390.3 MB 13.3 MB/s eta 0:00:18\n",
      "   --------------- ----------------------- 159.9/390.3 MB 13.4 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 161.7/390.3 MB 13.2 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 161.7/390.3 MB 13.2 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 161.7/390.3 MB 13.2 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 164.1/390.3 MB 12.7 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 165.2/390.3 MB 12.7 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 168.3/390.3 MB 12.6 MB/s eta 0:00:18\n",
      "   ---------------- ---------------------- 169.9/390.3 MB 12.5 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 171.4/390.3 MB 12.5 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 173.0/390.3 MB 12.4 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 175.4/390.3 MB 12.4 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 176.9/390.3 MB 12.3 MB/s eta 0:00:18\n",
      "   ----------------- --------------------- 178.8/390.3 MB 12.3 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 180.6/390.3 MB 12.2 MB/s eta 0:00:18\n",
      "   ------------------ -------------------- 183.5/390.3 MB 12.2 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 185.1/390.3 MB 12.2 MB/s eta 0:00:17\n",
      "   ------------------ -------------------- 188.5/390.3 MB 12.2 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 190.8/390.3 MB 12.2 MB/s eta 0:00:17\n",
      "   ------------------- ------------------- 194.2/390.3 MB 12.3 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 195.3/390.3 MB 12.2 MB/s eta 0:00:16\n",
      "   ------------------- ------------------- 199.2/390.3 MB 12.3 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 202.4/390.3 MB 12.3 MB/s eta 0:00:16\n",
      "   -------------------- ------------------ 205.8/390.3 MB 12.3 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 210.2/390.3 MB 12.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 213.4/390.3 MB 12.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 215.5/390.3 MB 12.5 MB/s eta 0:00:15\n",
      "   --------------------- ----------------- 217.8/390.3 MB 12.5 MB/s eta 0:00:14\n",
      "   --------------------- ----------------- 219.9/390.3 MB 12.4 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 222.0/390.3 MB 12.4 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 225.2/390.3 MB 12.4 MB/s eta 0:00:14\n",
      "   ---------------------- ---------------- 228.6/390.3 MB 12.5 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 232.0/390.3 MB 12.5 MB/s eta 0:00:13\n",
      "   ----------------------- --------------- 236.2/390.3 MB 12.6 MB/s eta 0:00:13\n",
      "   ------------------------ -------------- 240.6/390.3 MB 12.7 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 244.1/390.3 MB 12.7 MB/s eta 0:00:12\n",
      "   ------------------------ -------------- 247.5/390.3 MB 12.8 MB/s eta 0:00:12\n",
      "   ------------------------- ------------- 250.6/390.3 MB 12.8 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 254.5/390.3 MB 12.9 MB/s eta 0:00:11\n",
      "   ------------------------- ------------- 258.2/390.3 MB 12.9 MB/s eta 0:00:11\n",
      "   -------------------------- ------------ 261.4/390.3 MB 12.9 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 263.5/390.3 MB 12.9 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 265.8/390.3 MB 12.8 MB/s eta 0:00:10\n",
      "   -------------------------- ------------ 269.0/390.3 MB 12.8 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 272.1/390.3 MB 12.9 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 273.7/390.3 MB 12.8 MB/s eta 0:00:10\n",
      "   --------------------------- ----------- 275.8/390.3 MB 12.8 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 278.4/390.3 MB 12.8 MB/s eta 0:00:09\n",
      "   --------------------------- ----------- 280.0/390.3 MB 12.7 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 282.9/390.3 MB 12.7 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 286.3/390.3 MB 12.9 MB/s eta 0:00:09\n",
      "   ---------------------------- ---------- 288.4/390.3 MB 13.0 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 288.4/390.3 MB 13.0 MB/s eta 0:00:08\n",
      "   ---------------------------- ---------- 288.9/390.3 MB 12.8 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 292.0/390.3 MB 12.8 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 296.2/390.3 MB 12.9 MB/s eta 0:00:08\n",
      "   ----------------------------- --------- 299.9/390.3 MB 12.9 MB/s eta 0:00:08\n",
      "   ------------------------------ -------- 303.8/390.3 MB 12.9 MB/s eta 0:00:07\n",
      "   ------------------------------ -------- 307.0/390.3 MB 12.9 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 310.9/390.3 MB 13.0 MB/s eta 0:00:07\n",
      "   ------------------------------- ------- 315.1/390.3 MB 12.9 MB/s eta 0:00:06\n",
      "   ------------------------------- ------- 320.1/390.3 MB 13.0 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 324.5/390.3 MB 13.0 MB/s eta 0:00:06\n",
      "   -------------------------------- ------ 328.7/390.3 MB 13.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 333.2/390.3 MB 13.0 MB/s eta 0:00:05\n",
      "   --------------------------------- ----- 337.1/390.3 MB 13.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ---- 340.8/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 345.2/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 347.3/390.3 MB 13.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ---- 350.0/390.3 MB 13.0 MB/s eta 0:00:04\n",
      "   ----------------------------------- --- 353.1/390.3 MB 13.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 355.7/390.3 MB 13.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- --- 359.1/390.3 MB 13.2 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 361.5/390.3 MB 13.4 MB/s eta 0:00:03\n",
      "   ------------------------------------ -- 365.7/390.3 MB 13.6 MB/s eta 0:00:02\n",
      "   ------------------------------------ -- 369.4/390.3 MB 13.6 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 371.2/390.3 MB 13.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 371.5/390.3 MB 13.5 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 372.5/390.3 MB 13.3 MB/s eta 0:00:02\n",
      "   ------------------------------------- - 375.9/390.3 MB 13.3 MB/s eta 0:00:02\n",
      "   --------------------------------------  380.4/390.3 MB 13.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  383.8/390.3 MB 13.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  386.9/390.3 MB 13.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  389.8/390.3 MB 13.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 13.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 13.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  390.1/390.3 MB 13.3 MB/s eta 0:00:01\n",
      "   --------------------------------------- 390.3/390.3 MB 12.8 MB/s eta 0:00:00\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-25.1.24-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading grpcio-1.70.0-cp312-cp312-win_amd64.whl (4.3 MB)\n",
      "   ---------------------------------------- 0.0/4.3 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.6/4.3 MB 12.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.3/4.3 MB 11.8 MB/s eta 0:00:00\n",
      "Using cached keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Downloading ml_dtypes-0.4.1-cp312-cp312-win_amd64.whl (127 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Downloading optree-0.14.0-cp312-cp312-win_amd64.whl (299 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, termcolor, tensorboard-data-server, optree, opt-einsum, ml-dtypes, grpcio, google-pasta, gast, astunparse, absl-py, tensorboard, keras, tensorflow-intel, tensorflow\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-25.1.24 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 keras-3.8.0 libclang-18.1.1 ml-dtypes-0.4.1 namex-0.0.8 opt-einsum-3.4.0 optree-0.14.0 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 termcolor-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1caec6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3f7972a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "artist    0\n",
      "song      0\n",
      "link      0\n",
      "text      0\n",
      "dtype: int64\n",
      "\n",
      "Missing values after preprocessing:\n",
      "artist    0\n",
      "song      0\n",
      "link      0\n",
      "text      0\n",
      "dtype: int64\n",
      "\n",
      "Cleaned dataset saved to: C:\\Users\\STSC\\Downloads\\cleaned_spotify_millsongdata.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\STSC\\Downloads\\spotify_millsongdata.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Drop rows with missing values (if applicable)\n",
    "df_cleaned = df.dropna()\n",
    "\n",
    "# Fill missing values\n",
    "if df.select_dtypes(include=['float64', 'int64']).shape[1] > 0:  \n",
    "    for col in df_cleaned.select_dtypes(include=['float64', 'int64']).columns:\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mean())\n",
    "\n",
    "if df.select_dtypes(include=['object']).shape[1] > 0:  \n",
    "    for col in df_cleaned.select_dtypes(include=['object']).columns:\n",
    "        df_cleaned[col] = df_cleaned[col].fillna(df_cleaned[col].mode()[0])\n",
    "\n",
    "# Verify missing values after preprocessing\n",
    "missing_values_after = df_cleaned.isnull().sum()\n",
    "print(\"\\nMissing values after preprocessing:\")\n",
    "print(missing_values_after)\n",
    "\n",
    "# Save the cleaned dataset\n",
    "cleaned_file_path = r\"C:\\Users\\STSC\\Downloads\\cleaned_spotify_millsongdata.csv\"\n",
    "df_cleaned.to_csv(cleaned_file_path, index=False)\n",
    "\n",
    "print(f\"\\nCleaned dataset saved to: {cleaned_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30611b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "artist    0\n",
      "song      0\n",
      "link      0\n",
      "text      0\n",
      "dtype: int64\n",
      "\n",
      "No numerical columns to fill.\n",
      "No numerical columns available for normalization.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\STSC\\Downloads\\spotify_millsongdata.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = df.isnull().sum()\n",
    "print(\"Missing values per column:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Fill missing values for numerical columns\n",
    "numerical_cols = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "if len(numerical_cols) > 0:\n",
    "    for col in numerical_cols:\n",
    "        df[col] = df[col].fillna(df[col].mean())\n",
    "    print(\"\\nMissing values in numerical columns filled with mean.\")\n",
    "else:\n",
    "    print(\"\\nNo numerical columns to fill.\")\n",
    "\n",
    "# Normalize numerical features using Min-Max Scaling\n",
    "if len(numerical_cols) > 0:\n",
    "    scaler = MinMaxScaler()\n",
    "    df[numerical_cols] = scaler.fit_transform(df[numerical_cols])\n",
    "\n",
    "    print(\"\\nNormalized Data (first 5 rows):\")\n",
    "    print(df.head())\n",
    "\n",
    "    # Save the normalized dataset\n",
    "    normalized_file_path = r\"C:\\Users\\STSC\\Downloads\\normalized_spotify_millsongdata.csv\"\n",
    "    df.to_csv(normalized_file_path, index=False)\n",
    "    print(f\"\\nNormalized dataset saved to: {normalized_file_path}\")\n",
    "else:\n",
    "    print(\"No numerical columns available for normalization.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d9fa75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the dataset:\n",
      "Index(['artist', 'song', 'link', 'text'], dtype='object')\n",
      "\n",
      "Sample data:\n",
      "  artist                   song                                        link  \\\n",
      "0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n",
      "1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n",
      "2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n",
      "3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n",
      "4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n",
      "\n",
      "                                                text  \n",
      "0  Look at her face, it's a wonderful face  \\r\\nA...  \n",
      "1  Take it easy with me, please  \\r\\nTouch me gen...  \n",
      "2  I'll never know why I had to go  \\r\\nWhy I had...  \n",
      "3  Making somebody happy is a question of give an...  \n",
      "4  Making somebody happy is a question of give an...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\STSC\\Downloads\\spotify_millsongdata.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Display the columns and inspect the data\n",
    "print(\"Columns in the dataset:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Check a few rows of the data to identify categorical columns\n",
    "print(\"\\nSample data:\")\n",
    "print(df.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e82e59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Encoding completed. Sample data:\n",
      "  artist  artist_encoded\n",
      "0   ABBA               1\n",
      "1   ABBA               1\n",
      "2   ABBA               1\n",
      "3   ABBA               1\n",
      "4   ABBA               1\n",
      "\n",
      "Updated dataset saved to: C:\\Users\\STSC\\Downloads\\encoded_spotify_millsongdata.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\STSC\\Downloads\\spotify_millsongdata.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Check if 'artist' column exists\n",
    "if 'artist' in df.columns:\n",
    "    # Initialize LabelEncoder\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # Encode the 'artist' column\n",
    "    df['artist_encoded'] = label_encoder.fit_transform(df['artist'])\n",
    "    \n",
    "    print(\"\\nEncoding completed. Sample data:\")\n",
    "    print(df[['artist', 'artist_encoded']].head())\n",
    "else:\n",
    "    print(\"\\nColumn 'artist' not found in the dataset.\")\n",
    "\n",
    "# Optionally, save the updated dataset\n",
    "encoded_file_path = r\"C:\\Users\\STSC\\Downloads\\encoded_spotify_millsongdata.csv\"\n",
    "df.to_csv(encoded_file_path, index=False)\n",
    "print(f\"\\nUpdated dataset saved to: {encoded_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7103bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m   1/1802\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:10\u001b[0m 106ms/step"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lasyatummala/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1802/1802\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175us/step\n",
      "\n",
      "Embeddings for 'artist' variable:\n",
      "[[-0.04180036 -0.01407305 -0.01465716 -0.02176708 -0.00671035]\n",
      " [-0.04180036 -0.01407305 -0.01465716 -0.02176708 -0.00671035]\n",
      " [-0.04180036 -0.01407305 -0.01465716 -0.02176708 -0.00671035]\n",
      " [-0.04180036 -0.01407305 -0.01465716 -0.02176708 -0.00671035]\n",
      " [-0.04180036 -0.01407305 -0.01465716 -0.02176708 -0.00671035]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset from the uploaded file\n",
    "df = pd.read_csv('/Users/lasyatummala/Downloads/spotify_millsongdata.csv')\n",
    "\n",
    "# Encode the 'artist' column (you can choose another categorical column if needed)\n",
    "label_encoder = LabelEncoder()\n",
    "df['artist_encoded'] = label_encoder.fit_transform(df['artist'])\n",
    "\n",
    "# Define the embedding dimension (you can adjust this as needed)\n",
    "embedding_dim = 5  # You can change this depending on the dataset and task\n",
    "\n",
    "# Create an embedding layer for the 'artist' column\n",
    "embedding_layer = Embedding(input_dim=len(label_encoder.classes_), output_dim=embedding_dim, input_length=1)\n",
    "\n",
    "# Reshape the input to have a batch dimension (required by Keras)\n",
    "artist_input = np.array(df['artist_encoded']).reshape(-1, 1)\n",
    "\n",
    "# Initialize a simple Keras model with the embedding layer\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())  # Flatten the output from the embedding layer\n",
    "\n",
    "# Transform the 'artist_encoded' data into embeddings using the model\n",
    "embeddings = model.predict(artist_input)\n",
    "\n",
    "# Print the embeddings\n",
    "print(\"\\nEmbeddings for 'artist' variable:\")\n",
    "print(embeddings[:5])  # Display the first 5 embeddings\n",
    "\n",
    "# Optionally, add the embeddings back to the DataFrame\n",
    "embedding_columns = [f'artist_embedding_{i}' for i in range(embedding_dim)]\n",
    "df[embedding_columns] = embeddings\n",
    "\n",
    "# Save the DataFrame with the embeddings to a new CSV\n",
    "df.to_csv('/Users/lasyatummala/Downloads/spotify_millsongdata_with_embeddings.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e66fc3c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top Songs Globally (Popularity-based):\n",
      "             artist  popularity_score\n",
      "0      Donna Summer               191\n",
      "1  Gordon Lightfoot               189\n",
      "2         Bob Dylan               188\n",
      "3     George Strait               188\n",
      "4      Loretta Lynn               187\n",
      "\n",
      "Top Songs by Genre (Rock):\n",
      "        artist  popularity_score\n",
      "0  The Beatles               178\n",
      "1        Queen               163\n",
      "\n",
      "Global popularity recommendations saved to: C:\\Users\\STSC\\Downloads\\global_popularity_recommendations.csv\n",
      "Genre-based recommendations saved to: C:\\Users\\STSC\\Downloads\\genre_popularity_recommendations.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\STSC\\Downloads\\spotify_millsongdata.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Global Popularity-based Recommendation\n",
    "# Assuming songs with more unique artists are more popular globally\n",
    "song_popularity = df['artist'].value_counts().reset_index()\n",
    "song_popularity.columns = ['artist', 'popularity_score']  # Popularity score based on artist frequency\n",
    "\n",
    "# Step 2: Genre-based Recommendation (Example for \"Rock\" genre)\n",
    "# User preference for \"Rock\" genre\n",
    "user_preference_genre = \"Rock\"\n",
    "\n",
    "# Example mapping: Manually defining some rock artists (normally, a genre column would exist)\n",
    "rock_artists = ['AC/DC', 'Led Zeppelin', 'Queen', 'The Beatles']  # Example rock artists\n",
    "\n",
    "# Filter songs by user-preferred genre (e.g., 'Rock')\n",
    "df_genre_preference = df[df['artist'].isin(rock_artists)]\n",
    "\n",
    "# Rank the songs by their popularity in the selected genre\n",
    "genre_popularity = df_genre_preference['artist'].value_counts().reset_index()\n",
    "genre_popularity.columns = ['artist', 'popularity_score']\n",
    "\n",
    "# Display the top songs globally and by genre\n",
    "print(\"\\nTop Songs Globally (Popularity-based):\")\n",
    "print(song_popularity.head())  # Display top 5 most popular artists globally\n",
    "\n",
    "print(\"\\nTop Songs by Genre (Rock):\")\n",
    "print(genre_popularity.head())  # Display top 5 most popular songs in the 'Rock' genre\n",
    "\n",
    "# Save the recommendations (optional)\n",
    "global_popularity_file = r\"C:\\Users\\STSC\\Downloads\\global_popularity_recommendations.csv\"\n",
    "genre_popularity_file = r\"C:\\Users\\STSC\\Downloads\\genre_popularity_recommendations.csv\"\n",
    "\n",
    "song_popularity.to_csv(global_popularity_file, index=False)\n",
    "genre_popularity.to_csv(genre_popularity_file, index=False)\n",
    "\n",
    "print(f\"\\nGlobal popularity recommendations saved to: {global_popularity_file}\")\n",
    "print(f\"Genre-based recommendations saved to: {genre_popularity_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "26b8e5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision@5: 0.40\n",
      "Recall@5: 0.67\n"
     ]
    }
   ],
   "source": [
    "# Example: Recommended top 5 songs based on popularity\n",
    "recommended_songs = ['Song A', 'Song B', 'Song C', 'Song D', 'Song E']\n",
    "\n",
    "# User's relevant songs (for comparison)\n",
    "relevant_songs = ['Song A', 'Song D', 'Song F']\n",
    "\n",
    "# Function to calculate Precision@K and Recall@K\n",
    "def precision_recall_at_k(recommended, relevant, k=5):\n",
    "    recommended_at_k = set(recommended[:k])\n",
    "    relevant_set = set(relevant)\n",
    "    \n",
    "    # Precision@K: Relevant recommended items out of K recommendations\n",
    "    precision_at_k = len(recommended_at_k & relevant_set) / k\n",
    "    \n",
    "    # Recall@K: Relevant recommended items out of all relevant items\n",
    "    recall_at_k = len(recommended_at_k & relevant_set) / len(relevant) if len(relevant) > 0 else 0\n",
    "    \n",
    "    return precision_at_k, recall_at_k\n",
    "\n",
    "# Compute Precision@K and Recall@K for K=5\n",
    "precision, recall = precision_recall_at_k(recommended_songs, relevant_songs, k=5)\n",
    "\n",
    "# Print results\n",
    "print(f\"Precision@5: {precision:.2f}\")\n",
    "print(f\"Recall@5: {recall:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82b82e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top K Popular Songs (based on artist frequency):\n",
      "             artist  popularity_score\n",
      "0      Donna Summer               191\n",
      "1  Gordon Lightfoot               189\n",
      "2         Bob Dylan               188\n",
      "3     George Strait               188\n",
      "4      Loretta Lynn               187\n",
      "\n",
      "Precision@5: 0.00\n",
      "Recall@5: 0.00\n",
      "\n",
      "Popularity-based recommendations saved to: C:\\Users\\STSC\\Downloads\\popularity_based_recommendations.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\STSC\\Downloads\\spotify_millsongdata.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Calculate Song Popularity (based on artist frequency)\n",
    "song_popularity = df['artist'].value_counts().reset_index()\n",
    "song_popularity.columns = ['artist', 'popularity_score']  # Popularity score based on artist frequency\n",
    "\n",
    "# Step 2: Recommend Top K songs based on popularity\n",
    "top_k = 5  # You can change K based on how many songs you want to recommend\n",
    "\n",
    "# Get the top K popular songs globally (based on frequency of artists)\n",
    "top_k_songs = song_popularity.head(top_k)\n",
    "print(\"\\nTop K Popular Songs (based on artist frequency):\")\n",
    "print(top_k_songs)\n",
    "\n",
    "# Step 3: Evaluate Popularity-Based Recommendation (Precision@K and Recall@K)\n",
    "# Assume a user's relevant songs (in a real-world scenario, this would be actual user data)\n",
    "relevant_songs = ['Song A', 'Song D', 'Song F']  # Replace this with actual user's relevant songs\n",
    "\n",
    "# Example: Top K recommended songs\n",
    "recommended_songs = top_k_songs['artist'].tolist()  # Get the artist names\n",
    "\n",
    "# Function to calculate Precision@K and Recall@K\n",
    "def precision_recall_at_k(recommended, relevant, k):\n",
    "    recommended_at_k = set(recommended[:k])\n",
    "    relevant_set = set(relevant)\n",
    "\n",
    "    # Precision@K: Relevant recommended items out of K recommendations\n",
    "    precision_at_k = len(recommended_at_k & relevant_set) / k\n",
    "\n",
    "    # Recall@K: Relevant recommended items out of all relevant items\n",
    "    recall_at_k = len(recommended_at_k & relevant_set) / len(relevant) if len(relevant) > 0 else 0\n",
    "\n",
    "    return precision_at_k, recall_at_k\n",
    "\n",
    "# Compute Precision@K and Recall@K\n",
    "precision, recall = precision_recall_at_k(recommended_songs, relevant_songs, top_k)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(f\"\\nPrecision@{top_k}: {precision:.2f}\")\n",
    "print(f\"Recall@{top_k}: {recall:.2f}\")\n",
    "\n",
    "# Save recommendations (optional)\n",
    "popularity_recommendation_file = r\"C:\\Users\\STSC\\Downloads\\popularity_based_recommendations.csv\"\n",
    "top_k_songs.to_csv(popularity_recommendation_file, index=False)\n",
    "print(f\"\\nPopularity-based recommendations saved to: {popularity_recommendation_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8643825f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Randomly Recommended Songs:\n",
      "['Wishbone Ash', 'Aerosmith', 'Fall Out Boy', 'Janis Joplin', 'Moody Blues']\n",
      "\n",
      "Random Precision@5: 0.00\n",
      "Random Recall@5: 0.00\n"
     ]
    }
   ],
   "source": [
    "# Random Recommendation Model\n",
    "\n",
    "# Shuffle the dataset and randomly pick top K songs\n",
    "random_recommended_songs = df.sample(n=top_k, random_state=42)['artist'].tolist()\n",
    "\n",
    "print(\"\\nRandomly Recommended Songs:\")\n",
    "print(random_recommended_songs)\n",
    "\n",
    "# Evaluate Random Recommendation (Precision@K and Recall@K)\n",
    "random_precision_at_k = len(set(random_recommended_songs[:top_k]) & set(relevant_songs)) / top_k\n",
    "print(f\"\\nRandom Precision@{top_k}: {random_precision_at_k:.2f}\")\n",
    "\n",
    "random_recall_at_k = len(set(random_recommended_songs[:top_k]) & set(relevant_songs)) / len(relevant_songs)\n",
    "print(f\"Random Recall@{top_k}: {random_recall_at_k:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "00c5c6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation of Popularity-Based Model:\n",
      "Precision@5: 0.00\n",
      "Recall@5: 0.00\n",
      "MAP@5: 0.00\n",
      "\n",
      "Evaluation of Random Recommendation Model:\n",
      "Precision@5: 0.00\n",
      "Recall@5: 0.00\n",
      "MAP@5: 0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\STSC\\Downloads\\spotify_millsongdata.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Step 1: Popularity-Based Recommendation (Top K based on frequency of artist)\n",
    "song_popularity = df['artist'].value_counts().reset_index()\n",
    "song_popularity.columns = ['artist', 'popularity_score']\n",
    "\n",
    "# Top K recommendations\n",
    "top_k = 5\n",
    "top_k_songs = song_popularity.head(top_k)\n",
    "recommended_songs_pop = top_k_songs['artist'].tolist()\n",
    "\n",
    "# Step 2: Random Recommendation Model (Randomly pick K songs)\n",
    "recommended_songs_random = df.sample(n=top_k, random_state=42)['artist'].tolist()\n",
    "\n",
    "# Step 3: Evaluation Metrics\n",
    "\n",
    "# Assume relevant songs (Replace with actual user data)\n",
    "relevant_songs = ['Song A', 'Song D', 'Song F']\n",
    "\n",
    "# Precision@K\n",
    "def precision_at_k(recommended, relevant, k):\n",
    "    return len(set(recommended[:k]) & set(relevant)) / k\n",
    "\n",
    "# Recall@K\n",
    "def recall_at_k(recommended, relevant, k):\n",
    "    return len(set(recommended[:k]) & set(relevant)) / len(relevant)\n",
    "\n",
    "# Mean Average Precision (MAP) instead of AUC\n",
    "def mean_average_precision(recommended, relevant, k):\n",
    "    hits = 0\n",
    "    sum_precisions = 0\n",
    "    for i, song in enumerate(recommended[:k]):\n",
    "        if song in relevant:\n",
    "            hits += 1\n",
    "            sum_precisions += hits / (i + 1)  # Precision at i+1\n",
    "    return sum_precisions / min(len(relevant), k) if len(relevant) > 0 else 0\n",
    "\n",
    "# Evaluate for Popularity-Based Model\n",
    "precision_popularity = precision_at_k(recommended_songs_pop, relevant_songs, top_k)\n",
    "recall_popularity = recall_at_k(recommended_songs_pop, relevant_songs, top_k)\n",
    "map_popularity = mean_average_precision(recommended_songs_pop, relevant_songs, top_k)\n",
    "\n",
    "# Evaluate for Random Model\n",
    "precision_random = precision_at_k(recommended_songs_random, relevant_songs, top_k)\n",
    "recall_random = recall_at_k(recommended_songs_random, relevant_songs, top_k)\n",
    "map_random = mean_average_precision(recommended_songs_random, relevant_songs, top_k)\n",
    "\n",
    "# Output evaluation results\n",
    "print(\"\\nEvaluation of Popularity-Based Model:\")\n",
    "print(f\"Precision@{top_k}: {precision_popularity:.2f}\")\n",
    "print(f\"Recall@{top_k}: {recall_popularity:.2f}\")\n",
    "print(f\"MAP@{top_k}: {map_popularity:.2f}\")\n",
    "\n",
    "print(\"\\nEvaluation of Random Recommendation Model:\")\n",
    "print(f\"Precision@{top_k}: {precision_random:.2f}\")\n",
    "print(f\"Recall@{top_k}: {recall_random:.2f}\")\n",
    "print(f\"MAP@{top_k}: {map_random:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e0c0e087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Collaborative Filtering (SVD) Evaluation:\n",
      "Precision@5: 0.00\n",
      "Recall@5: 0.00\n",
      "MAP@5: 0.00\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\STSC\\Downloads\\spotify_millsongdata.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "### Step 1: Prepare User-Song Interaction Matrix ###\n",
    "# Simulate user interaction matrix (if explicit user-song ratings are not available)\n",
    "# Assuming each artist represents a unique user (this should be replaced with actual user data)\n",
    "df['user_id'] = df['artist'].astype('category').cat.codes\n",
    "df['song_id'] = df['song'].astype('category').cat.codes\n",
    "\n",
    "# Create user-song interaction matrix\n",
    "user_song_matrix = pd.crosstab(df['user_id'], df['song_id'])\n",
    "user_song_sparse = csr_matrix(user_song_matrix)\n",
    "\n",
    "### Step 2: Apply Singular Value Decomposition (SVD) for Collaborative Filtering ###\n",
    "svd = TruncatedSVD(n_components=5, random_state=42)  # Adjust n_components as needed\n",
    "svd_matrix = svd.fit_transform(user_song_sparse)\n",
    "\n",
    "### Step 3: Reconstruct the Matrix for Predictions ###\n",
    "reconstructed_matrix = np.dot(svd_matrix, svd.components_)\n",
    "\n",
    "# Get top K recommended songs for a user (Assuming User 0)\n",
    "top_k = 5\n",
    "user_idx = 0  # Replace with actual user ID\n",
    "\n",
    "recommended_song_indices = np.argsort(reconstructed_matrix[user_idx])[-top_k:][::-1]\n",
    "recommended_songs_svd = df[df['song_id'].isin(recommended_song_indices)]['song'].unique().tolist()\n",
    "\n",
    "### Step 4: Evaluate Collaborative Filtering Recommendations ###\n",
    "# Assume relevant songs (Replace with actual user data)\n",
    "relevant_songs = ['Song A', 'Song D', 'Song F']\n",
    "\n",
    "# Evaluation Metrics\n",
    "def precision_at_k(recommended, relevant, k):\n",
    "    return len(set(recommended[:k]) & set(relevant)) / k\n",
    "\n",
    "def recall_at_k(recommended, relevant, k):\n",
    "    return len(set(recommended[:k]) & set(relevant)) / len(relevant)\n",
    "\n",
    "def mean_average_precision(recommended, relevant, k):\n",
    "    hits = 0\n",
    "    sum_precisions = 0\n",
    "    for i, song in enumerate(recommended[:k]):\n",
    "        if song in relevant:\n",
    "            hits += 1\n",
    "            sum_precisions += hits / (i + 1)  # Precision at i+1\n",
    "    return sum_precisions / min(len(relevant), k) if len(relevant) > 0 else 0\n",
    "\n",
    "# Evaluate SVD-Based Collaborative Filtering Model\n",
    "precision_svd = precision_at_k(recommended_songs_svd, relevant_songs, top_k)\n",
    "recall_svd = recall_at_k(recommended_songs_svd, relevant_songs, top_k)\n",
    "map_svd = mean_average_precision(recommended_songs_svd, relevant_songs, top_k)\n",
    "\n",
    "print(\"\\nCollaborative Filtering (SVD) Evaluation:\")\n",
    "print(f\"Precision@{top_k}: {precision_svd:.2f}\")\n",
    "print(f\"Recall@{top_k}: {recall_svd:.2f}\")\n",
    "print(f\"MAP@{top_k}: {map_svd:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4df52d1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Songs Based on Content Similarity:\n",
      "['What Kind Of Girl', 'The Kind Of Girl I Could Love', 'Girl (Why You Wanna Make Me Blue)', 'Girl', 'Not That Kind Of Love']\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "file_path = r\"C:\\Users\\STSC\\Downloads\\spotify_millsongdata.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Handle missing values in the 'text' column (Lyrics)\n",
    "df['text'] = df['text'].fillna(\"\")\n",
    "\n",
    "# Step 1: TF-IDF Vectorization\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=5000)  # Limit features to reduce memory usage\n",
    "tfidf_matrix = tfidf.fit_transform(df['text'])\n",
    "\n",
    "# Step 2: Use Nearest Neighbors for Similarity Search\n",
    "nn_model = NearestNeighbors(n_neighbors=6, metric=\"cosine\", algorithm=\"brute\")  # Using brute force for accuracy\n",
    "nn_model.fit(tfidf_matrix)\n",
    "\n",
    "# Step 3: Recommend top K most similar songs for a given song\n",
    "top_k = 5\n",
    "song_index = 0  # Example: Recommend based on the first song\n",
    "\n",
    "# Find nearest neighbors\n",
    "distances, indices = nn_model.kneighbors(tfidf_matrix[song_index], n_neighbors=top_k + 1)\n",
    "\n",
    "# Extract song recommendations (excluding itself)\n",
    "recommended_songs_content = df.iloc[indices[0][1:]]['song'].tolist()\n",
    "\n",
    "# Display recommended songs\n",
    "print(\"Recommended Songs Based on Content Similarity:\")\n",
    "print(recommended_songs_content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0a5234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVD Recommendations: ['Song_329', 'Song_437', 'Song_419', 'Song_338', 'Song_418']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1️⃣ **Simulate a User-Song Interaction Matrix**\n",
    "np.random.seed(42)\n",
    "num_users = 100   # Simulated users\n",
    "num_songs = 500   # Simulated songs\n",
    "\n",
    "# Generate random interactions (ratings from 1 to 5, with some missing values)\n",
    "interaction_matrix = np.random.randint(0, 6, size=(num_users, num_songs))\n",
    "\n",
    "# Convert to DataFrame\n",
    "user_song_df = pd.DataFrame(interaction_matrix, columns=[f\"Song_{i}\" for i in range(num_songs)])\n",
    "\n",
    "# 2️⃣ **Apply SVD for Recommendations**\n",
    "def svd_recommendations(user_index, top_n=5):\n",
    "    \"\"\"Recommend songs for a user using SVD (from sklearn).\"\"\"\n",
    "    \n",
    "    # Normalize Data (Standard Scaling)\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    interaction_matrix_scaled = scaler.fit_transform(interaction_matrix)\n",
    "\n",
    "    # Apply Truncated SVD (Reduced to 20 latent factors)\n",
    "    svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "    user_factors = svd.fit_transform(interaction_matrix_scaled)\n",
    "\n",
    "    # Predict scores for all songs for the user\n",
    "    user_ratings = np.dot(user_factors[user_index], svd.components_)\n",
    "    \n",
    "    # Get top N song recommendations\n",
    "    top_songs_idx = np.argsort(user_ratings)[::-1][:top_n]\n",
    "    return [f\"Song_{idx}\" for idx in top_songs_idx]\n",
    "\n",
    "# Run SVD-based recommendations for a sample user (User 0)\n",
    "user_id = 0\n",
    "print(\"SVD Recommendations:\", svd_recommendations(user_id, top_n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8fc9b83f-9137-4e52-8de6-9ecc549d1f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - loss: 9.0213 - mae: 2.6226\n",
      "Epoch 2/5\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.0161 - mae: 1.2217\n",
      "Epoch 3/5\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.9297 - mae: 1.2012\n",
      "Epoch 4/5\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8963 - mae: 1.1882\n",
      "Epoch 5/5\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8833 - mae: 1.1852\n",
      "NCF Recommendations: ['Song_64', 'Song_148', 'Song_66', 'Song_79', 'Song_279']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Dense, Concatenate\n",
    "\n",
    "# 1️⃣ **Simulate a User-Song Interaction Dataset**\n",
    "np.random.seed(42)\n",
    "num_users = 100   # Simulated users\n",
    "num_songs = 500   # Simulated songs\n",
    "\n",
    "# Generate random interactions (ratings from 1 to 5)\n",
    "user_ids = np.random.randint(0, num_users, 5000)\n",
    "song_ids = np.random.randint(0, num_songs, 5000)\n",
    "ratings = np.random.randint(1, 6, 5000)  # Ratings from 1 to 5\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame({'user': user_ids, 'song': song_ids, 'rating': ratings})\n",
    "\n",
    "# 2️⃣ **Neural Collaborative Filtering Model**\n",
    "embedding_dim = 16  # Size of embeddings\n",
    "\n",
    "# Input layers\n",
    "user_input = Input(shape=(1,))\n",
    "song_input = Input(shape=(1,))\n",
    "\n",
    "# Embedding layers\n",
    "user_embedding = Embedding(input_dim=num_users, output_dim=embedding_dim)(user_input)\n",
    "song_embedding = Embedding(input_dim=num_songs, output_dim=embedding_dim)(song_input)\n",
    "\n",
    "# Flatten embeddings\n",
    "user_flat = Flatten()(user_embedding)\n",
    "song_flat = Flatten()(song_embedding)\n",
    "\n",
    "# Concatenate embeddings\n",
    "concat = Concatenate()([user_flat, song_flat])\n",
    "\n",
    "# MLP layers\n",
    "dense1 = Dense(64, activation='relu')(concat)\n",
    "dense2 = Dense(32, activation='relu')(dense1)\n",
    "dense3 = Dense(16, activation='relu')(dense2)\n",
    "output = Dense(1, activation='linear')(dense3)  # Predict rating\n",
    "\n",
    "# Compile model\n",
    "model = Model(inputs=[user_input, song_input], outputs=output)\n",
    "model.compile(loss='mse', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "# 3️⃣ **Train the Model**\n",
    "X_train = [df['user'].values, df['song'].values]\n",
    "y_train = df['rating'].values\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=64, verbose=1)\n",
    "\n",
    "# 4️⃣ **Generate Song Recommendations for a User**\n",
    "def recommend_songs(user_id, top_n=5):\n",
    "    \"\"\"Recommend songs for a user using the trained model.\"\"\"\n",
    "    song_candidates = np.arange(num_songs)\n",
    "    user_input = np.full_like(song_candidates, user_id)\n",
    "\n",
    "    # Predict ratings for all songs\n",
    "    predicted_ratings = model.predict([user_input, song_candidates], verbose=0).flatten()\n",
    "\n",
    "    # Get top N song recommendations\n",
    "    top_songs_idx = np.argsort(predicted_ratings)[::-1][:top_n]\n",
    "    return [f\"Song_{idx}\" for idx in top_songs_idx]\n",
    "\n",
    "# Test recommendations for User 0\n",
    "print(\"NCF Recommendations:\", recommend_songs(user_id=0, top_n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f1128328-48f6-4dfc-bb94-84ffd64f98f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hybrid Recommendations: ['Song_199', 'Song_377', 'Song_338', 'Song_329', 'Song_418']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1️⃣ **Simulate a Dataset with Lyrics & User-Song Interactions**\n",
    "np.random.seed(42)\n",
    "num_users = 100\n",
    "num_songs = 500\n",
    "\n",
    "# Generate random user-song interactions (ratings 1-5)\n",
    "interaction_matrix = np.random.randint(0, 6, size=(num_users, num_songs))\n",
    "user_song_df = pd.DataFrame(interaction_matrix, columns=[f\"Song_{i}\" for i in range(num_songs)])\n",
    "\n",
    "# Generate random song lyrics data\n",
    "lyrics = [\" \".join(np.random.choice([\"love\", \"heart\", \"music\", \"dance\", \"night\", \"feel\", \"dream\"], 50)) for _ in range(num_songs)]\n",
    "songs_df = pd.DataFrame({'song_id': range(num_songs), 'lyrics': lyrics})\n",
    "\n",
    "# 2️⃣ **Content-Based Filtering (TF-IDF + Cosine Similarity)**\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(songs_df['lyrics'])\n",
    "\n",
    "# Compute cosine similarity between songs\n",
    "content_similarity = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 3️⃣ **Collaborative Filtering (SVD)**\n",
    "def svd_recommendations(user_index, top_n=5):\n",
    "    \"\"\"Collaborative filtering using SVD.\"\"\"\n",
    "    \n",
    "    # Normalize data\n",
    "    scaler = StandardScaler(with_mean=True, with_std=True)\n",
    "    interaction_scaled = scaler.fit_transform(interaction_matrix)\n",
    "\n",
    "    # Apply SVD (Latent factor model)\n",
    "    svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "    user_factors = svd.fit_transform(interaction_scaled)\n",
    "\n",
    "    # Predict ratings\n",
    "    user_ratings = np.dot(user_factors[user_index], svd.components_)\n",
    "\n",
    "    # Get top N recommendations\n",
    "    top_songs_idx = np.argsort(user_ratings)[::-1][:top_n]\n",
    "    return top_songs_idx, user_ratings\n",
    "\n",
    "# 4️⃣ **Hybrid Recommendation System**\n",
    "def hybrid_recommendation(user_index, song_index, content_weight=0.5, top_n=5):\n",
    "    \"\"\"Blends content-based similarity with collaborative filtering scores.\"\"\"\n",
    "    \n",
    "    # Get collaborative filtering scores\n",
    "    top_songs_idx, user_ratings = svd_recommendations(user_index, top_n=top_n)\n",
    "    \n",
    "    # Get content similarity scores for the input song\n",
    "    content_scores = content_similarity[song_index]\n",
    "    \n",
    "    # Normalize scores\n",
    "    content_scores = (content_scores - content_scores.min()) / (content_scores.max() - content_scores.min())\n",
    "    user_ratings = (user_ratings - user_ratings.min()) / (user_ratings.max() - user_ratings.min())\n",
    "\n",
    "    # Blend content-based & collaborative scores\n",
    "    hybrid_scores = content_weight * content_scores + (1 - content_weight) * user_ratings\n",
    "\n",
    "    # Get top N recommended songs\n",
    "    top_hybrid_songs_idx = np.argsort(hybrid_scores)[::-1][:top_n]\n",
    "    \n",
    "    return [f\"Song_{idx}\" for idx in top_hybrid_songs_idx]\n",
    "\n",
    "# 5️⃣ **Test Hybrid Recommendation for User 0 & Song 10**\n",
    "print(\"Hybrid Recommendations:\", hybrid_recommendation(user_index=0, song_index=10, content_weight=0.5, top_n=5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2064079-9b0e-4ca6-89c6-7c32f980bb67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'RMSE': 1.1832, 'Precision@K': 0.0, 'Recall@K': 0.0, 'NDCG@K': 0.0, 'Diversity': 0.0375, 'Novelty': 0.0837}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# 1️⃣ **RMSE (Root Mean Squared Error)**\n",
    "def compute_rmse(actual_ratings, predicted_ratings):\n",
    "    \"\"\"Calculate RMSE for rating predictions.\"\"\"\n",
    "    return np.sqrt(mean_squared_error(actual_ratings, predicted_ratings))\n",
    "\n",
    "# 2️⃣ **Precision@K & Recall@K**\n",
    "def precision_recall_at_k(recommended_songs, actual_songs, k=5):\n",
    "    \"\"\"Compute Precision@K and Recall@K.\"\"\"\n",
    "    recommended_top_k = recommended_songs[:k]\n",
    "    hits = len(set(recommended_top_k) & set(actual_songs))\n",
    "    \n",
    "    precision = hits / k\n",
    "    recall = hits / len(actual_songs) if actual_songs else 0\n",
    "    \n",
    "    return precision, recall\n",
    "\n",
    "# 3️⃣ **NDCG (Normalized Discounted Cumulative Gain)**\n",
    "def ndcg_at_k(recommended_songs, actual_songs, k=5):\n",
    "    \"\"\"Compute NDCG@K to evaluate ranking quality.\"\"\"\n",
    "    dcg = sum([(1 / np.log2(i + 2)) if recommended_songs[i] in actual_songs else 0 for i in range(k)])\n",
    "    idcg = sum([(1 / np.log2(i + 2)) for i in range(min(len(actual_songs), k))])\n",
    "    \n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# 4️⃣ **Diversity & Novelty Metrics**\n",
    "def diversity_novelty(recommended_songs, content_similarity_matrix):\n",
    "    \"\"\"Compute diversity & novelty scores.\"\"\"\n",
    "    if len(recommended_songs) < 2:\n",
    "        return 0, 0\n",
    "    \n",
    "    # Diversity: Measure dissimilarity among recommended songs\n",
    "    diversity = 1 - np.mean([content_similarity_matrix[i, j] for i in recommended_songs for j in recommended_songs if i != j])\n",
    "    \n",
    "    # Novelty: Assume low similarity to popular songs is more novel\n",
    "    popularity_scores = np.mean(content_similarity_matrix, axis=0)\n",
    "    novelty = 1 - np.mean([popularity_scores[song] for song in recommended_songs])\n",
    "    \n",
    "    return diversity, novelty\n",
    "\n",
    "# 5️⃣ **Evaluate Hybrid Recommendation System**\n",
    "def evaluate_recommendation(user_index, song_index, actual_songs, k=5):\n",
    "    \"\"\"Evaluate hybrid recommendations using multiple metrics.\"\"\"\n",
    "    \n",
    "    # Get hybrid recommendations\n",
    "    recommended_songs = hybrid_recommendation(user_index, song_index, content_weight=0.5, top_n=k)\n",
    "    \n",
    "    # Generate random ground truth ratings for evaluation (simulating real user feedback)\n",
    "    actual_ratings = np.random.randint(1, 6, size=k)\n",
    "    predicted_ratings = np.random.randint(1, 6, size=k)  # Simulated predictions\n",
    "\n",
    "    # Compute Metrics\n",
    "    rmse = compute_rmse(actual_ratings, predicted_ratings)\n",
    "    precision, recall = precision_recall_at_k(recommended_songs, actual_songs, k)\n",
    "    ndcg = ndcg_at_k(recommended_songs, actual_songs, k)\n",
    "    diversity, novelty = diversity_novelty([int(song.split(\"_\")[1]) for song in recommended_songs], content_similarity)\n",
    "    \n",
    "    # Display Results\n",
    "    results = {\n",
    "        \"RMSE\": round(rmse, 4),\n",
    "        \"Precision@K\": round(precision, 4),\n",
    "        \"Recall@K\": round(recall, 4),\n",
    "        \"NDCG@K\": round(ndcg, 4),\n",
    "        \"Diversity\": round(diversity, 4),\n",
    "        \"Novelty\": round(novelty, 4),\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 6️⃣ **Run Evaluation for User 0 & Song 10**\n",
    "actual_songs = [\"Song_12\", \"Song_45\", \"Song_200\", \"Song_312\"]  # Simulated ground truth liked songs\n",
    "evaluation_results = evaluate_recommendation(user_index=0, song_index=10, actual_songs=actual_songs, k=5)\n",
    "\n",
    "# Display evaluation metrics\n",
    "print(\"Evaluation Results:\", evaluation_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f13e33c5-ecf0-47b6-bf37-d06029d8dd53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 A/B Testing Results: {'CTR Strategy A (Hybrid)': 24.99, 'CTR Strategy B (Collaborative)': 20.05, 'Avg Clicks per User (A)': 124.94, 'Avg Clicks per User (B)': 100.25, 'P-Value (Significance)': 0.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1️⃣ **Simulate User Interactions**\n",
    "np.random.seed(42)\n",
    "num_users = 1000   # Simulated users\n",
    "num_songs = 500    # Simulated songs\n",
    "\n",
    "# Generate random interactions (1 = clicked/recommended song, 0 = ignored)\n",
    "# A/B Testing: Strategy A (Hybrid) vs. Strategy B (Collaborative Filtering)\n",
    "strategy_a_clicks = np.random.binomial(1, 0.25, size=(num_users, num_songs))  # 25% CTR\n",
    "strategy_b_clicks = np.random.binomial(1, 0.20, size=(num_users, num_songs))  # 20% CTR\n",
    "\n",
    "# Convert to DataFrame for analysis\n",
    "ab_test_df = pd.DataFrame({\n",
    "    'user_id': np.arange(num_users),\n",
    "    'strategy_a_clicks': np.sum(strategy_a_clicks, axis=1),\n",
    "    'strategy_b_clicks': np.sum(strategy_b_clicks, axis=1)\n",
    "})\n",
    "\n",
    "# 2️⃣ **Compute Click-Through Rate (CTR)**\n",
    "def compute_ctr(click_data):\n",
    "    \"\"\"Calculate CTR (Click-Through Rate).\"\"\"\n",
    "    total_recommendations = click_data.shape[1] * click_data.shape[0]\n",
    "    total_clicks = np.sum(click_data)\n",
    "    return total_clicks / total_recommendations\n",
    "\n",
    "ctr_a = compute_ctr(strategy_a_clicks)\n",
    "ctr_b = compute_ctr(strategy_b_clicks)\n",
    "\n",
    "# 3️⃣ **Evaluate Engagement (Average Clicks per User)**\n",
    "avg_clicks_a = np.mean(ab_test_df['strategy_a_clicks'])\n",
    "avg_clicks_b = np.mean(ab_test_df['strategy_b_clicks'])\n",
    "\n",
    "# 4️⃣ **Statistical Significance (Chi-Square Test)**\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "clicks = np.array([\n",
    "    [np.sum(strategy_a_clicks), np.size(strategy_a_clicks) - np.sum(strategy_a_clicks)],\n",
    "    [np.sum(strategy_b_clicks), np.size(strategy_b_clicks) - np.sum(strategy_b_clicks)]\n",
    "])\n",
    "\n",
    "chi2, p_value, _, _ = chi2_contingency(clicks)\n",
    "\n",
    "# 5️⃣ **Display A/B Testing Results**\n",
    "results = {\n",
    "    \"CTR Strategy A (Hybrid)\": round(ctr_a * 100, 2),\n",
    "    \"CTR Strategy B (Collaborative)\": round(ctr_b * 100, 2),\n",
    "    \"Avg Clicks per User (A)\": round(avg_clicks_a, 2),\n",
    "    \"Avg Clicks per User (B)\": round(avg_clicks_b, 2),\n",
    "    \"P-Value (Significance)\": round(p_value, 4)\n",
    "}\n",
    "\n",
    "print(\"📊 A/B Testing Results:\", results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "237387b6-56ee-40ad-b3bf-1ca904cde640",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-05 21:25:32.512 \n",
      "  \u001b[33m\u001b[1mWarning:\u001b[0m to view this Streamlit app on a browser, run it with the following\n",
      "  command:\n",
      "\n",
      "    streamlit run C:\\Users\\STSC\\anaconda3\\Lib\\site-packages\\ipykernel_launcher.py [ARGUMENTS]\n",
      "2025-02-05 21:25:32.513 Session state does not function when running a script without `streamlit run`\n"
     ]
    }
   ],
   "source": [
    "import streamlit as st\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 1️⃣ **Simulated Dataset**\n",
    "np.random.seed(42)\n",
    "num_users = 100\n",
    "num_songs = 500\n",
    "\n",
    "# Simulated user-song interaction matrix (ratings 1-5)\n",
    "interaction_matrix = np.random.randint(0, 6, size=(num_users, num_songs))\n",
    "\n",
    "# Generate random song lyrics for content-based filtering\n",
    "lyrics = [\" \".join(np.random.choice([\"love\", \"heart\", \"music\", \"dance\", \"night\", \"feel\", \"dream\"], 50)) for _ in range(num_songs)]\n",
    "songs_df = pd.DataFrame({'song_id': range(num_songs), 'lyrics': lyrics})\n",
    "\n",
    "# 2️⃣ **Content-Based Filtering (TF-IDF + Cosine Similarity)**\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
    "tfidf_matrix = vectorizer.fit_transform(songs_df['lyrics'])\n",
    "content_similarity = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# 3️⃣ **Collaborative Filtering (SVD)**\n",
    "def svd_recommendations(user_index, top_n=5):\n",
    "    \"\"\"Collaborative filtering using SVD.\"\"\"\n",
    "    user_factors = TruncatedSVD(n_components=20, random_state=42).fit_transform(interaction_matrix)\n",
    "    user_ratings = np.dot(user_factors[user_index], user_factors.T)\n",
    "    top_songs_idx = np.argsort(user_ratings)[::-1][:top_n]\n",
    "    return [f\"Song_{idx}\" for idx in top_songs_idx]\n",
    "\n",
    "# 4️⃣ **Hybrid Recommendation**\n",
    "def hybrid_recommendation(user_index, song_index, content_weight=0.5, top_n=5):\n",
    "    \"\"\"Blends content-based & collaborative filtering scores.\"\"\"\n",
    "    top_songs_idx, user_ratings = svd_recommendations(user_index, top_n=top_n), []\n",
    "    content_scores = content_similarity[song_index]\n",
    "    content_scores = (content_scores - content_scores.min()) / (content_scores.max() - content_scores.min())\n",
    "    hybrid_scores = content_weight * content_scores + (1 - content_weight) * np.random.rand(num_songs)\n",
    "    top_hybrid_songs_idx = np.argsort(hybrid_scores)[::-1][:top_n]\n",
    "    return [f\"Song_{idx}\" for idx in top_hybrid_songs_idx]\n",
    "\n",
    "# 5️⃣ **Streamlit Web App**\n",
    "st.title(\"🎵 Music Recommendation System\")\n",
    "\n",
    "# Select Recommendation Strategy\n",
    "strategy = st.selectbox(\"Select Recommendation Strategy\", [\"Hybrid\", \"Collaborative\", \"Content-Based\"])\n",
    "\n",
    "# Select User ID\n",
    "user_id = st.slider(\"Select User ID\", 0, num_users - 1, 0)\n",
    "\n",
    "# Select Song ID for Hybrid / Content-Based\n",
    "song_id = st.slider(\"Select Song ID\", 0, num_songs - 1, 10)\n",
    "\n",
    "# Generate Recommendations\n",
    "if strategy == \"Hybrid\":\n",
    "    recommendations = hybrid_recommendation(user_id, song_id, content_weight=0.5, top_n=5)\n",
    "elif strategy == \"Collaborative\":\n",
    "    recommendations = svd_recommendations(user_id, top_n=5)\n",
    "else:\n",
    "    recommendations = [f\"Song_{idx}\" for idx in np.argsort(content_similarity[song_id])[-5:][::-1]]\n",
    "\n",
    "# Display Recommendations\n",
    "st.subheader(f\"🎶 Recommended Songs for User {user_id}:\")\n",
    "st.write(recommendations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a7bb7d-4a1d-48f4-9f01-91ebe2c8d182",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
